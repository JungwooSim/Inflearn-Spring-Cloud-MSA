[2022-01-07 00:00:03,588] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:00:03,595] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:00:13,599] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:00:13,600] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:00:23,603] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:00:23,604] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:00:33,609] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:00:33,609] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:00:43,615] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:00:43,615] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:00:53,619] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:00:53,621] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:01:03,623] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:01:03,624] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:01:13,628] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:01:13,629] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:01:23,630] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:01:23,630] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:01:33,631] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:01:33,632] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:01:43,637] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:01:43,638] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:01:53,641] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:01:53,641] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:02:03,644] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:02:03,645] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:02:13,645] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:02:13,645] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:02:23,651] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:02:23,651] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:02:33,654] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:02:33,654] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:02:43,658] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:02:43,658] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:02:53,659] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:02:53,659] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:03:03,663] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:03:03,664] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:03:13,668] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:03:13,668] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:03:23,673] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:03:23,673] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:03:33,676] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:03:33,676] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:03:43,602] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:03:43,603] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:03:53,607] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:03:53,607] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:04:03,612] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:04:03,613] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:04:13,618] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:04:13,618] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:04:23,622] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:04:23,623] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:04:33,626] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:04:33,627] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:04:43,630] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:04:43,630] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:04:53,634] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:04:53,635] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:05:03,638] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:05:03,638] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:05:13,642] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:05:13,643] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:05:23,648] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:05:23,648] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:05:33,650] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:05:33,651] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:05:43,656] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:05:43,656] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:05:53,660] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:05:53,661] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:06:03,665] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:06:03,665] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:06:13,670] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:06:13,670] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:06:23,674] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:06:23,675] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:06:33,680] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:06:33,681] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:06:43,685] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:06:43,685] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:06:53,690] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:06:53,690] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:07:03,692] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:07:03,692] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:07:13,695] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:07:13,697] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:07:23,702] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:07:23,702] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:07:33,707] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:07:33,708] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:07:43,710] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:07:43,711] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:07:53,716] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:07:53,716] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:08:03,720] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:08:03,720] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:08:13,724] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:08:13,725] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:08:23,729] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:08:23,730] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:08:33,734] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:08:33,736] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:08:43,740] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:08:43,741] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:08:53,744] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:08:53,745] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:09:03,750] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:09:03,751] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:09:13,756] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:09:13,756] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:09:23,759] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:09:23,760] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:09:33,763] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:09:33,764] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:09:43,769] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:09:43,769] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:09:53,773] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:09:53,773] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:10:03,777] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:10:03,777] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:10:13,782] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:10:13,783] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:10:23,786] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:10:23,786] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:10:33,790] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:10:33,791] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:10:43,792] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:10:43,793] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:10:53,795] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:10:53,796] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:11:03,800] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:11:03,801] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:11:13,803] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:11:13,803] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:11:23,807] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:11:23,808] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:11:33,813] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:11:33,813] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:11:43,817] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:11:43,819] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:11:53,824] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:11:53,824] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:12:03,829] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:12:03,830] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:12:13,834] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:12:13,834] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:12:23,838] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:12:23,839] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:12:33,841] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:12:33,841] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:12:43,846] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:12:43,847] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:12:53,851] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:12:53,852] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:13:03,856] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:13:03,857] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:13:13,862] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:13:13,862] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:13:23,867] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:13:23,869] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:13:33,871] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:13:33,872] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:13:43,876] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:13:43,876] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:13:53,878] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:13:53,879] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:14:03,883] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:14:03,884] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:14:13,885] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:14:13,885] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:14:23,885] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:14:23,885] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:14:33,889] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:14:33,889] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:14:43,892] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:14:43,893] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:14:53,896] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:14:53,897] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:15:03,901] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:15:03,902] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:15:13,906] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:15:13,907] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:15:23,912] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:15:23,913] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:15:33,915] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:15:33,917] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:15:43,922] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:15:43,922] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:15:53,927] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:15:53,928] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:16:03,932] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:16:03,932] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:16:13,934] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:16:13,934] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:16:23,935] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:16:23,935] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:16:33,941] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:16:33,943] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:16:43,945] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:16:43,945] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:16:53,948] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:16:53,948] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:17:03,951] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:17:03,953] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:17:13,957] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:17:13,957] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:17:23,961] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:17:23,961] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:17:33,964] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:17:33,966] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:17:43,969] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:17:43,969] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:17:53,971] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:17:53,971] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:18:03,973] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:18:03,974] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:18:13,978] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:18:13,979] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:18:23,979] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:18:23,980] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:18:33,982] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:18:33,983] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:18:43,987] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:18:43,988] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:18:53,992] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:18:53,993] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:19:03,995] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:19:03,996] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:19:13,999] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:19:14,000] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:19:24,004] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:19:24,005] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:19:34,010] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:19:34,011] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:19:39,963] INFO [Worker clientId=connect-1, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1578)
[2022-01-07 00:19:44,016] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:19:44,016] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:19:54,021] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:19:54,022] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:20:04,023] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:20:04,023] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:20:14,027] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:20:14,027] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:20:24,029] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:20:24,030] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:20:34,033] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:20:34,034] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:20:44,038] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:20:44,039] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:20:54,043] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:20:54,043] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:21:04,043] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:21:04,044] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:21:14,049] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:21:14,049] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:21:24,052] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:21:24,053] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:21:34,058] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:21:34,058] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:21:44,062] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:21:44,062] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:21:54,067] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:21:54,067] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:22:04,071] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:22:04,073] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:22:14,077] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:22:14,078] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:22:24,081] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:22:24,081] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:22:34,085] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:22:34,086] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:22:44,090] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:22:44,090] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:22:54,094] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:22:54,094] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:23:04,099] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:23:04,099] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:23:14,103] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:23:14,103] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:23:24,104] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:23:24,105] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:23:34,108] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:23:34,109] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:23:44,113] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:23:44,113] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:23:54,117] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:23:54,117] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:24:04,121] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:24:04,121] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:24:14,126] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:24:14,126] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:24:24,128] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:24:24,129] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:24:34,134] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:24:34,136] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:24:44,140] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:24:44,141] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:24:54,145] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:24:54,145] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:25:04,150] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:25:04,150] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:25:14,153] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:25:14,153] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:25:24,157] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:25:24,158] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:25:34,161] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:25:34,161] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:25:44,164] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:25:44,165] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:25:54,169] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:25:54,170] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:26:04,172] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:26:04,172] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:26:14,176] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:26:14,177] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:26:24,182] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:26:24,182] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:26:34,186] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:26:34,186] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:26:44,189] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:26:44,189] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:26:54,194] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:26:54,194] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:27:04,198] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:27:04,199] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:27:14,201] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:27:14,202] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:27:24,207] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:27:24,207] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:27:34,211] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:27:34,211] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:27:44,213] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:27:44,213] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:27:54,217] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:27:54,218] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:28:04,222] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:28:04,223] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:28:14,226] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:28:14,227] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:28:24,232] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:28:24,233] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:28:34,234] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:28:34,234] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:28:44,237] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:28:44,237] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:28:54,241] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:28:54,241] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:29:04,244] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:29:04,245] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:29:14,250] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:29:14,250] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:29:24,253] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:29:24,253] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:29:34,258] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:29:34,259] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:29:44,264] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:29:44,266] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:29:54,270] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:29:54,271] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:30:04,274] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:30:04,275] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:30:14,279] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:30:14,280] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:30:24,283] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:30:24,283] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:30:34,286] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:30:34,288] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:30:44,290] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:30:44,291] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:30:54,292] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:30:54,292] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:31:04,296] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:31:04,296] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:31:12,151] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2022-01-07 00:31:12,159] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1534)
[2022-01-07 00:31:12,160] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:31:12,160] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:31:12,169] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=6, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:31:12,178] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=6, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:31:12,178] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=18, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:31:12,178] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 18 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:31:12,179] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:31:12,180] INFO Creating connector my-order-sink-connect of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:31:12,181] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:31:12,182] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:31:12,184] INFO Instantiated connector my-order-sink-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:31:12,185] INFO Finished creating connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:31:12,185] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:31:12,187] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:31:12,187] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:31:12,187] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2022-01-07 00:31:12,677] INFO [Worker clientId=connect-1, groupId=connect-cluster] Tasks [my-order-sink-connect-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1549)
[2022-01-07 00:31:12,677] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder:647)
[2022-01-07 00:31:12,678] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:31:12,678] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:31:12,681] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=7, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:31:12,684] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=7, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:31:12,685] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=20, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-order-sink-connect-0, my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:31:12,685] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 20 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:31:12,686] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:31:12,686] INFO Creating task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:31:12,688] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:31:12,688] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:31:12,689] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:31:12,689] INFO Instantiated task my-order-sink-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:31:12,691] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:31:12,691] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:31:12,692] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:31:12,692] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:31:12,692] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:31:12,694] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2022-01-07 00:31:12,694] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:31:12,694] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:31:12,695] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-my-order-sink-connect-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-order-sink-connect
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:31:12,706] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:31:12,706] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:31:12,706] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:31:12,706] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:31:12,706] INFO Kafka startTimeMs: 1641483072706 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:31:12,709] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:31:12,709] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Subscribed to topic(s): orders (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2022-01-07 00:31:12,709] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2022-01-07 00:31:12,710] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:361)
[2022-01-07 00:31:12,710] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:31:12,710] INFO WorkerSinkTask{id=my-order-sink-connect-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2022-01-07 00:31:12,805] WARN [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Error while fetching metadata with correlation id 2 : {orders=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1117)
[2022-01-07 00:31:12,806] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:31:12,807] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:31:12,808] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:31:12,815] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:31:12,818] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-my-order-sink-connect-0-c6430aaf-c8df-40fe-adaa-a1e5c69c95f2', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:31:12,910] WARN [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Error while fetching metadata with correlation id 7 : {orders=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1117)
[2022-01-07 00:31:12,912] WARN [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] The following subscribed topics are not assigned to any members: [orders]  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:611)
[2022-01-07 00:31:12,912] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Finished assignment for group at generation 1: {connector-consumer-my-order-sink-connect-0-c6430aaf-c8df-40fe-adaa-a1e5c69c95f2=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:31:12,917] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-my-order-sink-connect-0-c6430aaf-c8df-40fe-adaa-a1e5c69c95f2', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:31:12,918] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:31:12,918] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:31:13,012] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Requesting to re-join the group and trigger rebalance since the assignment metadata has changed from (version3: {}) to (version4: {orders=1}) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:772)
[2022-01-07 00:31:13,013] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Requesting to re-join the group and trigger rebalance since the assignment metadata has changed from (version3: {}) to (version4: {orders=1}) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:772)
[2022-01-07 00:31:13,017] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2022-01-07 00:31:13,017] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:31:13,019] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-my-order-sink-connect-0-c6430aaf-c8df-40fe-adaa-a1e5c69c95f2', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:31:13,019] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Finished assignment for group at generation 2: {connector-consumer-my-order-sink-connect-0-c6430aaf-c8df-40fe-adaa-a1e5c69c95f2=Assignment(partitions=[orders-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:31:13,022] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-my-order-sink-connect-0-c6430aaf-c8df-40fe-adaa-a1e5c69c95f2', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:31:13,022] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Notifying assignor about the new Assignment(partitions=[orders-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:31:13,022] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Adding newly assigned partitions: orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:31:13,026] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Found no committed offset for partition orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1354)
[2022-01-07 00:31:13,028] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Resetting offset for partition orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:31:14,300] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:31:14,300] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:31:24,302] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:31:24,302] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:31:34,306] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:31:34,306] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:31:44,310] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:31:44,311] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:31:54,314] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:31:54,315] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:32:04,319] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:32:04,320] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:32:14,323] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:32:14,324] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:32:24,328] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:32:24,328] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:32:34,331] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:32:34,331] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:32:44,335] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:32:44,335] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:32:54,338] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:32:54,338] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:33:04,343] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:33:04,344] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:33:14,346] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:33:14,346] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:33:24,347] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:33:24,347] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:33:34,351] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:33:34,351] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:33:44,355] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:33:44,355] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:33:54,358] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:33:54,359] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:34:04,362] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:34:04,362] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:34:14,367] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:34:14,368] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:34:24,370] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:34:24,371] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:34:34,375] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:34:34,376] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:34:44,380] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:34:44,380] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:34:54,385] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:34:54,386] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:35:04,389] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:35:04,391] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:35:14,395] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:35:14,396] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:35:24,398] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:35:24,400] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:35:26,861] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:26,877] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:26,909] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:26,912] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:27,613] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:27,615] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:27,624] WARN Write of 1 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=61) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=61) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:27,635] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:27,638] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:27,639] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=61) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=61) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=61) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=61) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:30,641] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:30,651] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:30,676] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:30,678] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:30,696] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:30,698] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:30,701] WARN Write of 1 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=63) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=63) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:30,702] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:30,705] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:30,705] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=63) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=63) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=63) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=63) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:32,726] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:32,739] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:32,752] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:32,755] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:32,774] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:32,775] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:32,778] WARN Write of 1 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=65) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=65) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:32,779] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:32,781] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:32,781] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=65) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=65) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=65) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=65) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:34,400] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:35:34,400] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:35:35,783] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:35,792] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:35,805] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:35,808] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:35,835] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:35,838] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:35,843] WARN Write of 1 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=67) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=67) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:35,844] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:35,846] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:35,846] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=67) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=67) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=67) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=67) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:38,848] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:38,857] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:38,870] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:38,873] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:38,901] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:38,904] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:38,909] WARN Write of 1 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=69) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=69) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:38,910] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:38,912] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:38,913] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=69) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=69) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=69) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=69) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:41,914] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:41,923] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:41,933] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:41,935] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:41,955] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:41,957] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:41,961] WARN Write of 1 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=71) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=71) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:41,962] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:41,963] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:41,964] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=71) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=71) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=71) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=71) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:42,782] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:42,790] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:42,797] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:42,799] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:42,815] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:42,817] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:42,820] WARN Write of 1 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=73) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=73) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:42,821] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:42,823] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:42,823] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=73) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=73) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=73) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=73) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:44,404] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:35:44,404] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:35:45,824] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:45,833] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:45,853] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:45,856] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:45,878] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:45,880] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:45,883] WARN Write of 1 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=75) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=75) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:45,884] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:45,887] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:45,887] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=75) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=75) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=75) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=75) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:48,888] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:48,904] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:48,917] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:48,919] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:48,939] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:48,941] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:48,943] WARN Write of 1 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=77) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=77) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:48,945] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:48,946] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:48,946] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=77) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=77) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=77) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=77) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:51,949] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:51,964] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:51,977] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:51,980] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:52,000] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:52,002] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:52,004] WARN Write of 1 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=79) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=79) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:52,005] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:52,006] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:35:52,006] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=79) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=79) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=79) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=79) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:52,824] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:35:52,836] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:35:52,850] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:35:52,852] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:35:52,869] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:35:52,871] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:35:52,874] WARN Write of 1 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=81) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=81) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:35:52,874] ERROR Failing task after exhausting retries; encountered 4 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:113)
[2022-01-07 00:35:52,874] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=81) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=81) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value
 (org.apache.kafka.connect.runtime.WorkerSinkTask:612)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=81) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=81) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:122)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=81) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=81) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:52,875] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:614)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=81) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=81) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:122)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	... 10 more
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=81) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=81) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:35:52,875] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2022-01-07 00:35:52,875] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:35:52,877] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Revoke previously assigned partitions orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2022-01-07 00:35:52,877] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Member connector-consumer-my-order-sink-connect-0-c6430aaf-c8df-40fe-adaa-a1e5c69c95f2 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2022-01-07 00:35:52,887] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:35:52,887] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:35:52,887] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:35:52,895] INFO App info kafka.consumer for connector-consumer-my-order-sink-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:35:54,408] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:35:54,409] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:36:04,413] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:36:04,414] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:36:14,417] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:36:14,418] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:36:24,423] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:36:24,424] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:36:34,429] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:36:34,436] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:36:44,439] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:36:44,440] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:36:54,444] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:36:54,444] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:37:04,448] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:37:04,449] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:37:14,454] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:37:14,456] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:37:24,460] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:37:24,462] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:37:34,466] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:37:34,467] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:37:44,472] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:37:44,473] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:37:54,478] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:37:54,478] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:38:04,483] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:38:04,484] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:38:14,488] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:38:14,489] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:38:24,494] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:38:24,495] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:38:34,499] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:38:34,499] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:38:44,504] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:38:44,505] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:38:54,508] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:38:54,508] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:39:04,510] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:39:04,510] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:39:14,514] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:39:14,514] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:39:24,515] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:39:24,515] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:39:34,516] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:39:34,517] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:39:44,520] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:39:44,520] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:39:54,524] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:39:54,525] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:40:04,528] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:40:04,528] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:40:14,533] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:40:14,533] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:40:24,544] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:40:24,545] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:40:34,558] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:40:34,558] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:40:44,568] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:40:44,569] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:40:54,576] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:40:54,577] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:41:04,579] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:41:04,579] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:41:14,585] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:41:14,586] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:41:24,591] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:41:24,592] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:41:34,594] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:41:34,594] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:41:44,598] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:41:44,598] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:41:54,603] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:41:54,603] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:42:04,609] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:42:04,611] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:42:14,613] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:42:14,613] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:42:24,617] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:42:24,618] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:42:34,624] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:42:34,624] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:42:44,630] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:42:44,631] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:42:54,634] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:42:54,635] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:43:04,640] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:43:04,641] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:43:14,646] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:43:14,648] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:43:24,653] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:43:24,654] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:43:34,659] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:43:34,660] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:43:44,663] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:43:44,664] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:43:54,668] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:43:54,670] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:44:04,673] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:44:04,674] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:44:14,676] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:44:14,677] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:44:24,682] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:44:24,684] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:44:34,686] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:44:34,687] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:44:44,690] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:44:44,691] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:44:54,694] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:44:54,694] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:45:04,698] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:45:04,698] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:45:14,700] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:45:14,701] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:45:24,703] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:45:24,704] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:45:25,969] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2022-01-07 00:45:34,706] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:45:34,707] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:45:44,710] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:45:44,710] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:45:54,713] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:45:54,715] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:46:04,720] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:46:04,722] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:46:14,722] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:46:14,723] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:46:24,725] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:46:24,725] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:46:34,726] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:46:34,727] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:46:44,732] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:46:44,733] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:46:54,738] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:46:54,738] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:47:04,743] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:47:04,744] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:47:14,750] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:47:14,751] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:47:24,756] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:47:24,756] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:47:34,761] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:47:34,762] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:47:44,763] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:47:44,764] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:47:54,764] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:47:54,765] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:47:59,949] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:47:59,956] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1521)
[2022-01-07 00:47:59,957] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling connector-only config update by stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:593)
[2022-01-07 00:47:59,957] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:47:59,957] INFO Scheduled shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:47:59,959] INFO Completed shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:47:59,962] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:47:59,962] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:47:59,964] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=8, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:47:59,971] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=8, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:47:59,978] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:47:59,978] INFO Stopping task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:47:59,979] WARN Ignoring stop request for unowned connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:390)
[2022-01-07 00:47:59,981] WARN Ignoring await stop request for non-present connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:415)
[2022-01-07 00:47:59,985] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1767)
[2022-01-07 00:47:59,993] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1786)
[2022-01-07 00:47:59,993] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=22, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[my-order-sink-connect], revokedTaskIds=[my-order-sink-connect-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:47:59,994] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 22 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:47:59,994] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:47:59,994] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:47:59,994] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:47:59,996] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=9, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:48:00,001] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=9, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:48:00,001] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=22, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:48:00,001] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 22 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:48:00,002] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:48:04,769] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:48:04,769] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:48:11,514] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2022-01-07 00:48:11,518] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1534)
[2022-01-07 00:48:11,518] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:48:11,518] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:48:11,520] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=10, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:48:11,524] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=10, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:48:11,524] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=23, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:48:11,525] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 23 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:48:11,528] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:48:11,528] INFO Creating connector my-order-sink-connect of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:48:11,529] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:48:11,529] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:48:11,535] INFO Instantiated connector my-order-sink-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:48:11,535] INFO Finished creating connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:48:11,536] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:48:11,536] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:48:11,536] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:48:11,536] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2022-01-07 00:48:12,032] INFO [Worker clientId=connect-1, groupId=connect-cluster] Tasks [my-order-sink-connect-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1549)
[2022-01-07 00:48:12,033] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder:647)
[2022-01-07 00:48:12,033] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:48:12,033] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:48:12,035] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=11, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:48:12,037] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=11, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:48:12,038] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=25, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-order-sink-connect-0, my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:48:12,038] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 25 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:48:12,039] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:48:12,040] INFO Creating task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:48:12,044] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:48:12,045] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:48:12,045] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:48:12,045] INFO Instantiated task my-order-sink-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:48:12,045] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:48:12,045] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:48:12,046] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:48:12,046] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:48:12,046] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:48:12,048] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2022-01-07 00:48:12,048] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:48:12,049] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:48:12,052] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-my-order-sink-connect-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-order-sink-connect
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:48:12,059] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:48:12,059] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:48:12,059] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:48:12,059] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:48:12,059] INFO Kafka startTimeMs: 1641484092059 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:48:12,060] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:48:12,061] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Subscribed to topic(s): orders (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2022-01-07 00:48:12,061] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2022-01-07 00:48:12,061] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:361)
[2022-01-07 00:48:12,061] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:48:12,062] INFO WorkerSinkTask{id=my-order-sink-connect-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2022-01-07 00:48:12,064] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:48:12,064] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:48:12,065] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:48:12,068] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:48:12,069] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-my-order-sink-connect-0-58e42cba-ee27-44dc-9d8e-e44f1844888a', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:48:12,070] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Finished assignment for group at generation 1: {connector-consumer-my-order-sink-connect-0-58e42cba-ee27-44dc-9d8e-e44f1844888a=Assignment(partitions=[orders-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:48:12,074] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-my-order-sink-connect-0-58e42cba-ee27-44dc-9d8e-e44f1844888a', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:48:12,074] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Notifying assignor about the new Assignment(partitions=[orders-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:48:12,074] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Adding newly assigned partitions: orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:48:12,075] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Found no committed offset for partition orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1354)
[2022-01-07 00:48:12,079] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Resetting offset for partition orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:48:12,101] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Error converting message value in topic 'orders' partition 0 at offset 13 and timestamp 1641483815977: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration. (org.apache.kafka.connect.runtime.WorkerSinkTask:547)
org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:370)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertValue(WorkerSinkTask.java:545)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:478)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:328)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-01-07 00:48:12,107] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:206)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:478)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:328)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:370)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertValue(WorkerSinkTask.java:545)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	... 13 more
[2022-01-07 00:48:12,108] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2022-01-07 00:48:12,109] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Revoke previously assigned partitions orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2022-01-07 00:48:12,110] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Member connector-consumer-my-order-sink-connect-0-58e42cba-ee27-44dc-9d8e-e44f1844888a sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2022-01-07 00:48:12,112] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:48:12,112] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:48:12,112] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:48:12,113] INFO App info kafka.consumer for connector-consumer-my-order-sink-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:48:14,774] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:48:14,775] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:48:24,779] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:48:24,779] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:48:34,782] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:48:34,783] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:48:44,787] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:48:44,788] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:48:54,793] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:48:54,794] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:49:04,799] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:49:04,800] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:49:06,720] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:49:06,720] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1521)
[2022-01-07 00:49:06,721] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling connector-only config update by stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:593)
[2022-01-07 00:49:06,721] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:49:06,721] INFO Scheduled shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:49:06,721] INFO Completed shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:49:06,721] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:49:06,722] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:49:06,724] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=12, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:49:06,726] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=12, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:49:06,729] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:49:06,729] WARN Ignoring stop request for unowned connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:390)
[2022-01-07 00:49:06,729] INFO Stopping task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:49:06,729] WARN Ignoring await stop request for non-present connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:415)
[2022-01-07 00:49:06,729] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1767)
[2022-01-07 00:49:06,733] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1786)
[2022-01-07 00:49:06,733] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=27, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[my-order-sink-connect], revokedTaskIds=[my-order-sink-connect-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:49:06,734] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 27 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:49:06,734] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:49:06,734] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:49:06,734] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:49:06,736] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=13, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:49:06,738] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=13, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:49:06,738] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=27, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:49:06,738] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 27 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:49:06,738] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:49:14,804] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:49:14,805] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:49:24,809] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:49:24,809] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:49:34,813] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:49:34,814] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:49:44,819] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:49:44,820] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:49:54,823] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:49:54,823] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:50:04,825] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:50:04,826] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:50:14,832] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:50:14,832] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:50:24,836] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:50:24,836] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:50:34,841] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:50:34,843] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:50:44,845] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:50:44,846] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:50:54,849] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:50:54,850] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:50:55,676] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2022-01-07 00:50:55,680] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1534)
[2022-01-07 00:50:55,698] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:50:55,698] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:50:55,701] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=14, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:50:55,703] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=14, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:50:55,704] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 14 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=28, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:50:55,704] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 28 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:50:55,704] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:50:55,704] INFO Creating connector my-order-sink-connect of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:50:55,705] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:50:55,705] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:50:55,705] INFO Instantiated connector my-order-sink-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:50:55,705] INFO Finished creating connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:50:55,705] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:50:55,706] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:50:55,706] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:50:55,706] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2022-01-07 00:50:56,197] INFO [Worker clientId=connect-1, groupId=connect-cluster] Tasks [my-order-sink-connect-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1549)
[2022-01-07 00:50:56,198] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder:647)
[2022-01-07 00:50:56,198] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:50:56,198] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:50:56,200] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=15, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:50:56,203] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=15, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:50:56,203] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 15 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=30, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-order-sink-connect-0, my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:50:56,204] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 30 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:50:56,204] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:50:56,205] INFO Creating task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:50:56,205] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:50:56,205] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:50:56,205] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:50:56,206] INFO Instantiated task my-order-sink-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:50:56,206] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:50:56,206] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:50:56,206] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:50:56,206] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:50:56,207] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:50:56,207] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2022-01-07 00:50:56,207] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:50:56,207] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:50:56,208] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-my-order-sink-connect-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-order-sink-connect
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:50:56,212] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:50:56,212] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:50:56,212] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:50:56,212] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:50:56,212] INFO Kafka startTimeMs: 1641484256212 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:50:56,213] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:50:56,214] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Subscribed to topic(s): orders (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2022-01-07 00:50:56,214] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2022-01-07 00:50:56,214] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:361)
[2022-01-07 00:50:56,214] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:50:56,215] INFO WorkerSinkTask{id=my-order-sink-connect-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2022-01-07 00:50:56,217] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:50:56,218] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:50:56,218] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:50:56,220] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:50:56,222] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-my-order-sink-connect-0-84f7b3bb-3f20-4124-b291-26b652d11a70', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:50:56,222] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Finished assignment for group at generation 3: {connector-consumer-my-order-sink-connect-0-84f7b3bb-3f20-4124-b291-26b652d11a70=Assignment(partitions=[orders-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:50:56,224] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-my-order-sink-connect-0-84f7b3bb-3f20-4124-b291-26b652d11a70', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:50:56,225] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Notifying assignor about the new Assignment(partitions=[orders-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:50:56,225] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Adding newly assigned partitions: orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:50:56,226] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Found no committed offset for partition orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1354)
[2022-01-07 00:50:56,229] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Resetting offset for partition orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:50:56,234] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Error converting message value in topic 'orders' partition 0 at offset 13 and timestamp 1641483815977: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration. (org.apache.kafka.connect.runtime.WorkerSinkTask:547)
org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:370)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertValue(WorkerSinkTask.java:545)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:478)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:328)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-01-07 00:50:56,234] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:206)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:478)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:328)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:370)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertValue(WorkerSinkTask.java:545)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	... 13 more
[2022-01-07 00:50:56,235] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2022-01-07 00:50:56,235] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Revoke previously assigned partitions orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2022-01-07 00:50:56,235] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Member connector-consumer-my-order-sink-connect-0-84f7b3bb-3f20-4124-b291-26b652d11a70 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2022-01-07 00:50:56,237] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:50:56,237] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:50:56,238] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:50:56,239] INFO App info kafka.consumer for connector-consumer-my-order-sink-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:51:04,855] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:51:04,856] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:51:14,860] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:51:14,860] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:51:24,862] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:51:24,862] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:51:27,307] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:51:27,307] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1521)
[2022-01-07 00:51:27,308] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling connector-only config update by stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:593)
[2022-01-07 00:51:27,308] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:51:27,308] INFO Scheduled shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:51:27,308] INFO Completed shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:51:27,309] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:51:27,309] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:51:27,311] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=16, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:51:27,313] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=16, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:51:27,317] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:51:27,317] WARN Ignoring stop request for unowned connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:390)
[2022-01-07 00:51:27,317] INFO Stopping task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:51:27,317] WARN Ignoring await stop request for non-present connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:415)
[2022-01-07 00:51:27,321] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1767)
[2022-01-07 00:51:27,325] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1786)
[2022-01-07 00:51:27,325] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 16 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=32, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[my-order-sink-connect], revokedTaskIds=[my-order-sink-connect-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:51:27,325] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 32 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:51:27,326] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:51:27,326] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:51:27,326] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:51:27,328] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=17, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:51:27,330] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=17, memberId='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:51:27,330] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 17 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=32, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:51:27,330] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 32 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:51:27,331] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:51:30,417] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Error sending fetch request (sessionId=205505760, epoch=68188) to node 0: (org.apache.kafka.clients.FetchSessionHandler:481)
org.apache.kafka.common.errors.DisconnectException
[2022-01-07 00:51:30,417] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Error sending fetch request (sessionId=1047534976, epoch=68194) to node 0: (org.apache.kafka.clients.FetchSessionHandler:481)
org.apache.kafka.common.errors.DisconnectException
[2022-01-07 00:51:30,417] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Error sending fetch request (sessionId=1190899856, epoch=64932) to node 0: (org.apache.kafka.clients.FetchSessionHandler:481)
org.apache.kafka.common.errors.DisconnectException
[2022-01-07 00:51:30,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Error sending fetch request (sessionId=623567431, epoch=68165) to node 0: (org.apache.kafka.clients.FetchSessionHandler:481)
org.apache.kafka.common.errors.DisconnectException
[2022-01-07 00:51:30,418] INFO [Worker clientId=connect-1, groupId=connect-cluster] Group coordinator localhost:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:904)
[2022-01-07 00:51:30,418] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Group coordinator localhost:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:904)
[2022-01-07 00:51:30,470] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,470] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,470] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,470] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,470] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,519] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,519] WARN [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,519] WARN [Producer clientId=connector-producer-my-source-connect-0] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,519] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,572] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,572] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,572] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,621] WARN [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,621] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,621] WARN [Producer clientId=connector-producer-my-source-connect-0] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,621] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,622] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,622] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,775] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,775] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,825] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,825] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,874] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,874] WARN [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,874] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,874] WARN [Producer clientId=connector-producer-my-source-connect-0] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:30,874] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,181] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,231] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,281] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,281] WARN [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,281] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,281] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,281] WARN [Producer clientId=connector-producer-my-source-connect-0] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,281] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,382] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,991] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,991] WARN [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,991] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:31,991] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:32,045] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:32,093] WARN [Producer clientId=connector-producer-my-source-connect-0] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:32,093] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:32,143] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:32,294] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:32,904] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:32,905] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:32,904] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:33,005] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:33,056] WARN [Producer clientId=connector-producer-my-source-connect-0] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:33,107] WARN [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:33,157] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:33,207] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:33,258] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:33,419] INFO [Worker clientId=connect-1, groupId=connect-cluster] Broker coordinator was unreachable for 3000ms. Revoking previous assignment Assignment{error=0, leader='connect-1-31e2bc31-6aa5-49cb-82dd-ccf34a08d701', leaderUrl='http://127.0.0.1:8083/', offset=32, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} to avoid running tasks while not being a member the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:139)
[2022-01-07 00:51:33,420] INFO Stopping connector my-source-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:51:33,420] INFO Stopping task my-source-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:51:33,420] INFO Stopping task my-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:51:33,420] INFO Stopping connector my-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:51:33,442] INFO Scheduled shutdown for WorkerConnector{id=my-source-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:51:33,444] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2022-01-07 00:51:33,445] INFO Scheduled shutdown for WorkerConnector{id=my-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:51:33,444] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2022-01-07 00:51:33,445] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:182)
[2022-01-07 00:51:33,445] INFO Closing connection #2 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:51:33,447] INFO Completed shutdown for WorkerConnector{id=my-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:51:33,447] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:149)
[2022-01-07 00:51:33,449] INFO Closing connection #2 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:51:33,454] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2022-01-07 00:51:33,453] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Revoke previously assigned partitions my_topic_users-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2022-01-07 00:51:33,468] INFO Closing connection #3 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:51:33,469] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:51:33,469] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:51:33,469] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:51:33,472] INFO Completed shutdown for WorkerConnector{id=my-source-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:51:33,472] INFO App info kafka.consumer for connector-consumer-my-sink-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:51:33,472] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:51:33,507] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:51:33,508] INFO [Producer clientId=connector-producer-my-source-connect-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1205)
[2022-01-07 00:51:33,512] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:51:33,544] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:51:33,544] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:51:33,545] INFO App info kafka.producer for connector-producer-my-source-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:51:33,545] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1767)
[2022-01-07 00:51:33,967] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:34,019] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:34,070] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:34,121] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:34,125] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:34,222] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:34,251] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:34,936] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:35,038] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:35,101] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:35,140] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:35,183] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:35,336] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:35,393] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:35,852] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:36,092] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:36,150] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:36,159] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:36,209] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:36,253] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:36,429] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:36,967] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:37,063] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:37,089] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:37,111] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:37,174] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:37,325] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:37,466] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:38,033] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:38,115] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:38,180] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:38,283] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:38,288] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:38,304] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:38,388] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:39,095] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:39,101] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:39,193] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:39,197] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:39,252] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:39,435] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:39,454] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:39,960] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:40,059] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:40,059] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:40,263] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:40,321] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:40,364] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:40,578] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:40,871] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,023] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,131] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,276] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,386] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,404] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,480] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,733] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,945] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:41,986] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:42,088] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:42,341] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:42,364] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:42,547] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:42,797] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:43,110] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:43,202] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:43,202] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:43,303] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:43,377] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:43,526] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:43,659] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:44,117] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:44,199] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2022-01-07 00:51:44,199] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2022-01-07 00:51:44,210] INFO Stopped http_8083@6601cc93{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2022-01-07 00:51:44,211] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2022-01-07 00:51:44,212] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2022-01-07 00:51:44,212] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:682)
[2022-01-07 00:51:44,269] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:44,269] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:44,314] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:44,328] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:44,579] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:44,726] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:45,184] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:45,189] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:45,285] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:45,285] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:45,460] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:45,566] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:45,739] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:46,045] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:46,298] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:46,348] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:46,348] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:46,405] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:46,609] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:46,801] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:47,211] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:47,312] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:47,411] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:47,513] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:47,623] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:47,641] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:47,766] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:48,225] WARN [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:48,275] WARN [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:48,389] WARN [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:48,580] WARN [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:48,683] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:48,741] WARN [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:48,987] WARN [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Connection to node 0 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:780)
[2022-01-07 00:51:49,216] ERROR [Worker clientId=connect-1, groupId=connect-cluster] Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:303)
org.apache.kafka.common.errors.InterruptException: Flush interrupted.
	at org.apache.kafka.clients.producer.KafkaProducer.flush(KafkaProducer.java:1129)
	at org.apache.kafka.connect.util.KafkaBasedLog.flush(KafkaBasedLog.java:225)
	at org.apache.kafka.connect.storage.KafkaStatusBackingStore.flush(KafkaStatusBackingStore.java:255)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder$RebalanceListener.onRevoked(DistributedHerder.java:1785)
	at org.apache.kafka.connect.runtime.distributed.WorkerCoordinator.poll(WorkerCoordinator.java:141)
	at org.apache.kafka.connect.runtime.distributed.WorkerGroupMember.poll(WorkerGroupMember.java:181)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.tick(DistributedHerder.java:438)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:295)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:230)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.RecordAccumulator.awaitFlushCompletion(RecordAccumulator.java:712)
	at org.apache.kafka.clients.producer.KafkaProducer.flush(KafkaProducer.java:1127)
	... 12 more
[2022-01-07 00:51:49,216] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:702)
[2022-01-07 00:51:49,219] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2022-01-07 00:52:08,256] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../logs, -Dlog4j.configuration=file:./bin/../etc/kafka/connect-log4j.properties
	jvm.spec = Homebrew, OpenJDK 64-Bit Server VM, 17.0.1, 17.0.1+1
	jvm.classpath = /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-transforms-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/zstd-jni-1.4.5-6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-core-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-api-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/zookeeper-3.5.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/rocksdbjni-5.18.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jopt-simple-5.0.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-log4j-appender-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-io-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-runtime-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/reflections-0.9.12.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/slf4j-log4j12-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs-test-sources.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-codec-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-mirror-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-common-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-server-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-mirror-client-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-util-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/argparse4j-0.7.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/activation-1.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-servlets-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-clients-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-logging_2.13-3.9.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/slf4j-api-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/javassist-3.25.0-GA.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-reflect-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-media-jaxb-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/maven-artifact-3.6.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-streams-test-utils-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-library-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-continuation-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/hk2-locator-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-collection-compat_2.13-2.2.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-module-paranamer-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/plexus-utils-3.2.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-server-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/aopalliance-repackaged-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/zookeeper-jute-3.5.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-transport-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-streams-examples-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/commons-cli-1.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/paranamer-2.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-tools-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-common-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-streams-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jaxb-api-2.3.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-container-servlet-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/metrics-core-2.2.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-client-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-streams-scala_2.13-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.activation-api-1.2.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/lz4-java-1.7.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-http-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/osgi-resource-locator-1.0.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-container-servlet-core-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.inject-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/audience-annotations-0.5.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/hk2-utils-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-resolver-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-hk2-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/hk2-api-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-handler-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-databind-2.10.5.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-basic-auth-extension-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-transport-native-epoll-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-security-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.annotation-api-1.3.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/snappy-java-1.1.7.7.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-transport-native-unix-common-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs-sources.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-json-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-servlet-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/mariadb-java-client-2.7.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs-javadoc.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-file-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-client-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-raft-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-buffer-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs-test.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/javassist-3.26.0-GA.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.validation-api-2.0.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/commons-lang3-3.8.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/common-config-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/common-metrics-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/slf4j-api-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/build-tools-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/common-utils-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-protobuf-provider-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-streams-avro-serde-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-logging-1.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-core-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlinx-coroutines-core-1.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-stdlib-1.4.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/rocksdbjni-5.18.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/gson-2.8.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-reflect-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-scripting-common-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jersey-common-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/protobuf-java-3.11.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/joda-time-2.9.9.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/json-20190722.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-schema-registry-client-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/slf4j-api-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-json-schema-provider-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/wire-runtime-3.2.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/animal-sniffer-annotations-1.18.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/wire-schema-3.2.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-connect-protobuf-converter-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/org.everit.json.schema-1.12.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/scala-library-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-connect-json-schema-converter-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/protobuf-java-util-3.11.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-connect-avro-data-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jakarta.ws.rs-api-2.1.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-json-schema-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-validator-1.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/failureaccess-1.0.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/okio-2.5.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/annotations-13.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-datatype-guava-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/error_prone_annotations-2.3.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-streams-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-protobuf-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-schema-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/swagger-annotations-1.6.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/avro-1.9.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/handy-uri-templates-2.1.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/osgi-resource-locator-1.0.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/validation-api-2.0.1.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-datatype-jdk8-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-connect-avro-converter-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jakarta.inject-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/checker-qual-2.8.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-module-parameter-names-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-avro-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-collections-3.2.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-streams-protobuf-serde-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-compress-1.19.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-databind-2.10.5.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-streams-json-schema-serde-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-scripting-jvm-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jakarta.annotation-api-1.3.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/j2objc-annotations-1.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/classgraph-4.8.21.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-stdlib-jdk7-1.3.71.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-json-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-script-runtime-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/re2j-1.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-scripting-compiler-embeddable-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-stdlib-common-1.3.71.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-stdlib-jdk8-1.3.71.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jsr305-3.0.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-datatype-joda-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-datatype-jsr310-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/guava-28.1-jre.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-digester-1.8.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-transforms-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/zstd-jni-1.4.5-6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-core-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-api-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/zookeeper-3.5.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/rocksdbjni-5.18.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-log4j-appender-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-io-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-runtime-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/reflections-0.9.12.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs-test-sources.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-codec-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-mirror-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-common-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-server-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-mirror-client-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-util-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-servlets-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-clients-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-logging_2.13-3.9.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/javassist-3.25.0-GA.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-reflect-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-media-jaxb-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-streams-test-utils-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-library-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-continuation-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-collection-compat_2.13-2.2.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-module-paranamer-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-server-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-transport-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-streams-examples-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-tools-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-common-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-streams-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-container-servlet-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-client-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-streams-scala_2.13-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/lz4-java-1.7.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-http-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-container-servlet-core-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-resolver-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-hk2-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/hk2-api-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-handler-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-databind-2.10.5.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-basic-auth-extension-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-security-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/snappy-java-1.1.7.7.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs-sources.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-json-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-servlet-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/mariadb-java-client-2.7.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs-javadoc.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-file-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-client-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-raft-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-buffer-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs-test.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/confluent-telemetry/*
	os.spec = Mac OS X, x86_64, 12.0.1
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2022-01-07 00:52:08,270] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed:90)
[2022-01-07 00:52:08,288] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/checker-qual-3.5.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,380] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/checker-qual-3.5.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,380] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:08,380] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:08,381] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:08,385] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/osdt_cert-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,405] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/osdt_cert-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,406] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/xmlparserv2-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,502] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/xmlparserv2-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,502] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ons-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,516] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ons-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,516] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/slf4j-api-1.7.30.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,523] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/slf4j-api-1.7.30.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,524] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/oraclepki-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,546] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/oraclepki-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,547] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/simplefan-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,553] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/simplefan-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,554] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/postgresql-42.2.19.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,609] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/postgresql-42.2.19.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,612] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/xdb-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,624] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/xdb-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,625] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/jtds-1.3.1.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,642] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/jtds-1.3.1.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,644] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ucp-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,728] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ucp-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,729] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/kafka-connect-jdbc-10.2.6.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,749] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/kafka-connect-jdbc-10.2.6.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,750] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:08,750] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:08,750] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/common-utils-6.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,755] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/common-utils-6.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:08,756] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ojdbc8-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:08,967] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ojdbc8-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:09,000] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/osdt_core-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:09,025] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/osdt_core-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:09,025] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/sqlite-jdbc-3.25.2.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:09,043] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/sqlite-jdbc-3.25.2.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:09,045] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/orai18n-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:09,055] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/orai18n-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:09,055] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/mssql-jdbc-8.4.1.jre8.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:52:09,222] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/mssql-jdbc-8.4.1.jre8.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:10,719] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@251a69d7 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,720] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,724] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,725] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,725] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,725] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,725] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,725] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,725] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,726] INFO Added plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,726] INFO Added plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,726] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,726] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,727] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,727] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,727] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,727] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,727] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,727] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,728] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,729] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,730] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,730] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,730] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:52:10,731] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,731] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,731] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,731] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,731] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,731] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,731] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,731] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'JsonSchemaConverter' and 'JsonSchema' to plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'ProtobufConverter' and 'Protobuf' to plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,732] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,733] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,734] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,734] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,734] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,734] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,735] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,735] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,735] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,735] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,735] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,735] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,735] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,736] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,736] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,736] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,736] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:52:10,736] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,736] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,736] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:52:10,771] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.path = [/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:361)
[2022-01-07 00:52:10,777] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:52:10,779] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:10,835] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,835] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,835] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,835] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,835] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:10,836] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:10,836] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:10,836] INFO Kafka startTimeMs: 1641484330836 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,021] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:52:11,022] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,028] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,029] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,029] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,037] INFO Logging initialized @3239ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2022-01-07 00:52:11,065] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2022-01-07 00:52:11,066] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2022-01-07 00:52:11,070] INFO jetty-9.4.33.v20201020; built: 2020-10-20T23:39:24.803Z; git: 1be68755656cef678b79a2ef1c2ebbca99e25420; jvm 17.0.1+1 (org.eclipse.jetty.server.Server:375)
[2022-01-07 00:52:11,086] INFO Started http_8083@661c46bc{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2022-01-07 00:52:11,086] INFO Started @3289ms (org.eclipse.jetty.server.Server:415)
[2022-01-07 00:52:11,099] INFO Advertised URI: http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2022-01-07 00:52:11,099] INFO REST server listening at http://127.0.0.1:8083/, advertising URL http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2022-01-07 00:52:11,100] INFO Advertised URI: http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2022-01-07 00:52:11,100] INFO REST admin endpoints at http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2022-01-07 00:52:11,100] INFO Advertised URI: http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2022-01-07 00:52:11,100] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:52:11,101] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,104] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,104] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,104] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,104] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,104] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,104] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,104] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,104] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,104] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,105] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,105] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,105] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,105] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,105] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,105] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,105] INFO Kafka startTimeMs: 1641484331105 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,115] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:52:11,115] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,118] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,118] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,118] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,121] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2022-01-07 00:52:11,125] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:52:11,125] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,128] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,128] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,129] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,129] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,129] INFO Kafka startTimeMs: 1641484331129 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,137] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:52:11,137] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,139] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,139] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,139] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,142] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,142] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,142] INFO Kafka startTimeMs: 1641484331142 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,221] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:52:11,222] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:52:11,222] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:52:11,223] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,225] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,225] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,226] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,226] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,226] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,226] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,226] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,227] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,227] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,227] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,227] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,227] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,227] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,229] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,229] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,229] INFO Kafka startTimeMs: 1641484331229 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,237] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:52:11,237] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,239] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,239] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,239] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,243] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:52:11,244] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,247] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,247] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,247] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,247] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,247] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,247] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,247] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,247] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,248] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,248] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,248] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,248] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,248] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,248] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,248] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,248] INFO Kafka startTimeMs: 1641484331248 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,256] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:52:11,256] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,257] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,258] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,258] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,260] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:52:11,260] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,262] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,262] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,262] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,262] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,263] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,263] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,263] INFO Kafka startTimeMs: 1641484331263 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,270] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:52:11,271] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,272] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,272] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,272] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,280] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:52:11,281] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,282] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,282] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,283] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,283] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,283] INFO Kafka startTimeMs: 1641484331283 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,290] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:52:11,291] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,292] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,292] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,292] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,305] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,305] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,305] INFO Kafka startTimeMs: 1641484331305 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,307] INFO Kafka Connect distributed worker initialization took 3037ms (org.apache.kafka.connect.cli.ConnectDistributed:128)
[2022-01-07 00:52:11,307] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2022-01-07 00:52:11,309] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2022-01-07 00:52:11,309] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:287)
[2022-01-07 00:52:11,309] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2022-01-07 00:52:11,309] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:127)
[2022-01-07 00:52:11,309] INFO Starting KafkaBasedLog with topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:128)
[2022-01-07 00:52:11,310] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,312] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,312] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,313] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,313] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,313] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,313] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,313] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,313] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,313] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,313] INFO Kafka startTimeMs: 1641484331313 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,341] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2022-01-07 00:52:11,351] INFO App info kafka.admin.client for adminclient-8 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,352] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,352] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,352] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,356] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:361)
[2022-01-07 00:52:11,369] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,369] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,369] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,370] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,371] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,371] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,371] INFO Kafka startTimeMs: 1641484331370 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,376] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:52:11,377] INFO [Producer clientId=producer-1] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:11,394] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,394] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2022-01-07 00:52:11,395] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,395] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2022-01-07 00:52:11,395] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,395] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,395] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,395] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,395] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,395] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,395] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,396] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,396] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,396] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,396] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,396] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,396] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2022-01-07 00:52:11,396] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,396] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,396] INFO Kafka startTimeMs: 1641484331396 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,402] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:11,407] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Subscribed to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer:1116)
[2022-01-07 00:52:11,415] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,416] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,416] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,416] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,416] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,416] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,416] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,416] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,417] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,418] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,418] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,418] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,450] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,451] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,452] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,453] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,453] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,453] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,453] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,453] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,453] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,453] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,453] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,486] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:164)
[2022-01-07 00:52:11,486] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:166)
[2022-01-07 00:52:11,486] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:129)
[2022-01-07 00:52:11,487] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2022-01-07 00:52:11,487] INFO Starting KafkaBasedLog with topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:128)
[2022-01-07 00:52:11,488] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,489] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,489] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,490] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,490] INFO Kafka startTimeMs: 1641484331489 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,502] INFO App info kafka.admin.client for adminclient-9 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,502] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,503] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,503] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,503] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:361)
[2022-01-07 00:52:11,506] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,506] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,506] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,506] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,506] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,506] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,507] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,507] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,507] INFO Kafka startTimeMs: 1641484331507 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,508] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:52:11,510] INFO [Producer clientId=producer-2] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:11,512] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,513] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,514] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,514] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,514] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,514] INFO Kafka startTimeMs: 1641484331514 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,518] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:11,519] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Subscribed to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer:1116)
[2022-01-07 00:52:11,519] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,520] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,520] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,520] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,520] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,527] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,527] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,528] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,528] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,528] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,554] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:164)
[2022-01-07 00:52:11,554] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:166)
[2022-01-07 00:52:11,555] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:265)
[2022-01-07 00:52:11,555] INFO Starting KafkaBasedLog with topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:128)
[2022-01-07 00:52:11,555] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:52:11,556] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,557] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,557] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,557] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,557] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,557] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,557] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,557] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,557] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,558] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,558] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,558] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,558] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,558] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,558] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:52:11,558] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,558] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,558] INFO Kafka startTimeMs: 1641484331558 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,574] INFO App info kafka.admin.client for adminclient-10 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:52:11,575] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:52:11,576] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:52:11,576] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:52:11,576] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:361)
[2022-01-07 00:52:11,579] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,579] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,579] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,579] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,579] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,579] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:11,580] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,581] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,581] INFO Kafka startTimeMs: 1641484331580 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,581] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:52:11,584] INFO [Producer clientId=producer-3] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:11,584] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,584] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,584] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:11,585] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:11,585] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:11,585] INFO Kafka startTimeMs: 1641484331585 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:11,589] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:11,591] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Subscribed to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1116)
[2022-01-07 00:52:11,591] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:52:11,598] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:52:11,605] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:52:11,606] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:52:11,606] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:52:11,606] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:164)
[2022-01-07 00:52:11,606] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:166)
[2022-01-07 00:52:11,606] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:280)
[2022-01-07 00:52:11,607] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:291)
[2022-01-07 00:52:11,615] INFO [Worker clientId=connect-1, groupId=connect-cluster] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:11,616] INFO [Worker clientId=connect-1, groupId=connect-cluster] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:52:11,618] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:52:11,618] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:52:11,631] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:52:11,776] INFO Started o.e.j.s.ServletContextHandler@5f254608{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2022-01-07 00:52:11,776] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2022-01-07 00:52:11,776] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2022-01-07 00:52:14,005] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=18, memberId='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:52:14,025] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=18, memberId='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:52:14,026] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 18 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', leaderUrl='http://127.0.0.1:8083/', offset=32, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:52:14,027] WARN [Worker clientId=connect-1, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1095)
[2022-01-07 00:52:14,027] INFO [Worker clientId=connect-1, groupId=connect-cluster] Current config state offset -1 is behind group assignment 32, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1158)
[2022-01-07 00:52:14,128] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 32 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1162)
[2022-01-07 00:52:14,128] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 32 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:52:14,131] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-source-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:52:14,131] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:52:14,133] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-sink-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:52:14,133] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-source-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:52:14,140] INFO Creating task my-source-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:52:14,140] INFO Creating connector my-sink-connect of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:52:14,140] INFO Creating connector my-source-connect of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:52:14,140] INFO Creating task my-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:52:14,142] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:52:14,142] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:361)
[2022-01-07 00:52:14,142] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:52:14,142] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:52:14,144] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:14,144] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:14,144] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:14,144] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:14,148] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:52:14,148] INFO Instantiated task my-sink-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:52:14,148] INFO Instantiated connector my-source-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:52:14,148] INFO Instantiated connector my-sink-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:52:14,149] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:52:14,149] INFO Finished creating connector my-sink-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:52:14,149] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:52:14,149] INFO Finished creating connector my-source-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:52:14,149] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:52:14,150] INFO Instantiated task my-source-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:52:14,150] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:52:14,150] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2022-01-07 00:52:14,150] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:52:14,150] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:52:14,151] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-source-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:52:14,151] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:52:14,152] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:52:14,152] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-source-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:52:14,152] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-source-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:52:14,155] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:361)
[2022-01-07 00:52:14,155] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2022-01-07 00:52:14,156] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:14,156] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:52:14,156] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:14,157] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:606)
[2022-01-07 00:52:14,159] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-my-source-connect-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:361)
[2022-01-07 00:52:14,160] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-my-sink-connect-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-sink-connect
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:52:14,164] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:14,165] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:52:14,165] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:14,165] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:14,165] INFO Kafka startTimeMs: 1641484334165 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:14,165] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:14,166] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:52:14,166] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:52:14,166] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:52:14,166] INFO Kafka startTimeMs: 1641484334166 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:52:14,168] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	dialect.name = 
	incrementing.column.name = id
	mode = incrementing
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [users]
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = my_topic_
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:361)
[2022-01-07 00:52:14,170] INFO [Producer clientId=connector-producer-my-source-connect-0] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:14,172] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:52:14,172] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:52:14,173] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2022-01-07 00:52:14,173] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	dialect.name = 
	incrementing.column.name = id
	mode = incrementing
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [users]
	tables = [`inflearn_spring_cloud`.`users`]
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = my_topic_
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:361)
[2022-01-07 00:52:14,174] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Subscribed to topic(s): my_topic_users (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2022-01-07 00:52:14,174] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2022-01-07 00:52:14,174] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2022-01-07 00:52:14,177] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:361)
[2022-01-07 00:52:14,180] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:52:14,182] INFO WorkerSinkTask{id=my-sink-connect-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2022-01-07 00:52:14,187] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:52:14,187] INFO [Worker clientId=connect-1, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1578)
[2022-01-07 00:52:14,189] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:52:14,190] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:52:14,196] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:52:14,199] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-my-sink-connect-0-12f96550-9d61-44b8-9399-08f2fdbc74ad', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:52:14,202] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Finished assignment for group at generation 3: {connector-consumer-my-sink-connect-0-12f96550-9d61-44b8-9399-08f2fdbc74ad=Assignment(partitions=[my_topic_users-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:52:14,206] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-my-sink-connect-0-12f96550-9d61-44b8-9399-08f2fdbc74ad', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:52:14,206] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Notifying assignor about the new Assignment(partitions=[my_topic_users-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:52:14,206] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Adding newly assigned partitions: my_topic_users-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:52:14,215] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Setting offset for partition my_topic_users-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2022-01-07 00:52:14,226] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2022-01-07 00:52:14,538] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:52:14,582] INFO Found offset {{table=users}=null, {protocol=1, table=inflearn_spring_cloud.users}={incrementing=8}} for partition {protocol=1, table=inflearn_spring_cloud.users} (io.confluent.connect.jdbc.source.JdbcSourceTask:196)
[2022-01-07 00:52:14,583] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2022-01-07 00:52:14,583] INFO WorkerSourceTask{id=my-source-connect-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)
[2022-01-07 00:52:14,586] INFO Begin using SQL query: SELECT * FROM `inflearn_spring_cloud`.`users` WHERE `inflearn_spring_cloud`.`users`.`id` > ? ORDER BY `inflearn_spring_cloud`.`users`.`id` ASC (io.confluent.connect.jdbc.source.TableQuerier:164)
[2022-01-07 00:52:14,692] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:52:14,692] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:14,692] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2022-01-07 00:52:14,747] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:361)
[2022-01-07 00:52:14,747] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:14,749] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:361)
[2022-01-07 00:52:14,749] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:52:24,176] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:52:24,176] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:52:34,181] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:52:34,183] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:52:44,189] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:52:44,190] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:52:54,194] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:52:54,195] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:53:04,198] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:53:04,199] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:53:14,204] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:53:14,205] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:53:24,210] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:53:24,211] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:53:34,217] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:53:34,218] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:53:44,220] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:53:44,220] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:53:54,223] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:53:54,223] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:54:04,228] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:54:04,230] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:54:14,236] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:54:14,237] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:54:24,242] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:54:24,242] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:54:32,299] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2022-01-07 00:54:32,306] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1534)
[2022-01-07 00:54:32,307] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:54:32,307] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:54:32,310] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=19, memberId='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:54:32,312] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=19, memberId='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:54:32,313] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 19 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', leaderUrl='http://127.0.0.1:8083/', offset=34, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:54:32,313] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 34 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:54:32,314] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:54:32,314] INFO Creating connector my-order-sink-connect of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:54:32,314] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:54:32,314] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:54:32,315] INFO Instantiated connector my-order-sink-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:54:32,315] INFO Finished creating connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:54:32,315] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:54:32,316] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:54:32,316] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:54:32,317] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2022-01-07 00:54:32,822] INFO [Worker clientId=connect-1, groupId=connect-cluster] Tasks [my-order-sink-connect-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1549)
[2022-01-07 00:54:32,824] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder:647)
[2022-01-07 00:54:32,824] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:54:32,824] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:54:32,827] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=20, memberId='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:54:32,831] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=20, memberId='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:54:32,831] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 20 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1', leaderUrl='http://127.0.0.1:8083/', offset=36, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-order-sink-connect-0, my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:54:32,832] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 36 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:54:32,833] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:54:32,833] INFO Creating task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:54:32,834] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:54:32,834] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:54:32,835] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:54:32,835] INFO Instantiated task my-order-sink-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:54:32,835] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:54:32,836] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:54:32,836] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:54:32,836] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:54:32,837] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:54:32,837] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2022-01-07 00:54:32,838] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:54:32,838] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:54:32,839] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-my-order-sink-connect-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-order-sink-connect
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:54:32,843] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:54:32,843] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:54:32,843] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:54:32,843] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:54:32,843] INFO Kafka startTimeMs: 1641484472843 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:54:32,844] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:54:32,845] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Subscribed to topic(s): orders (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2022-01-07 00:54:32,845] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2022-01-07 00:54:32,845] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:361)
[2022-01-07 00:54:32,846] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:54:32,846] INFO WorkerSinkTask{id=my-order-sink-connect-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2022-01-07 00:54:32,851] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:54:32,851] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:54:32,853] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:54:32,856] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:54:32,857] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-my-order-sink-connect-0-2a977cc0-1fe9-44da-a65e-f6ccb984707b', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:54:32,858] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Finished assignment for group at generation 1: {connector-consumer-my-order-sink-connect-0-2a977cc0-1fe9-44da-a65e-f6ccb984707b=Assignment(partitions=[orders-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:54:32,860] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-my-order-sink-connect-0-2a977cc0-1fe9-44da-a65e-f6ccb984707b', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:54:32,860] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Notifying assignor about the new Assignment(partitions=[orders-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:54:32,860] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Adding newly assigned partitions: orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:54:32,861] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Found no committed offset for partition orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1354)
[2022-01-07 00:54:32,864] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Resetting offset for partition orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:54:32,876] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Error converting message value in topic 'orders' partition 0 at offset 13 and timestamp 1641483815977: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration. (org.apache.kafka.connect.runtime.WorkerSinkTask:547)
org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:370)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertValue(WorkerSinkTask.java:545)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:478)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:328)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-01-07 00:54:32,878] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:206)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:478)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:328)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:370)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertValue(WorkerSinkTask.java:545)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	... 13 more
[2022-01-07 00:54:32,879] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2022-01-07 00:54:32,880] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Revoke previously assigned partitions orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2022-01-07 00:54:32,880] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Member connector-consumer-my-order-sink-connect-0-2a977cc0-1fe9-44da-a65e-f6ccb984707b sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2022-01-07 00:54:32,886] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:54:32,886] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:54:32,886] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:54:32,887] INFO App info kafka.consumer for connector-consumer-my-order-sink-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:54:34,247] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:54:34,247] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:54:44,252] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:54:44,252] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:54:54,258] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:54:54,259] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:55:04,262] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:55:04,262] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:55:14,264] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:55:14,264] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:55:24,266] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:55:24,267] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:55:34,272] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:55:34,272] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:55:44,277] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:55:44,277] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:55:54,280] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:55:54,281] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:56:04,285] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:56:04,287] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:56:14,292] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:56:14,292] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:56:24,293] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:56:24,293] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:56:34,294] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:56:34,295] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:56:44,298] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:56:44,299] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:56:54,302] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:56:54,303] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:57:04,305] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:57:04,306] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:57:14,311] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:57:14,312] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:57:24,317] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:57:24,317] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:57:34,321] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:57:34,321] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:57:44,326] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:57:44,327] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:57:54,330] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:57:54,331] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:58:04,335] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:58:04,336] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:58:14,340] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:58:14,340] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:58:24,346] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:58:24,346] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:58:24,545] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2022-01-07 00:58:24,545] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2022-01-07 00:58:24,551] INFO Stopped http_8083@661c46bc{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2022-01-07 00:58:24,551] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2022-01-07 00:58:24,553] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2022-01-07 00:58:24,553] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:682)
[2022-01-07 00:58:24,553] INFO [Worker clientId=connect-1, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:656)
[2022-01-07 00:58:24,555] INFO Stopping connector my-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:58:24,555] INFO Scheduled shutdown for WorkerConnector{id=my-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:58:24,555] INFO Stopping task my-source-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:58:24,555] INFO Stopping task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:58:24,555] INFO Stopping task my-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:58:24,555] INFO Stopping connector my-source-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:58:24,556] INFO Scheduled shutdown for WorkerConnector{id=my-source-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:58:24,555] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:58:24,557] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:182)
[2022-01-07 00:58:24,556] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2022-01-07 00:58:24,556] INFO Completed shutdown for WorkerConnector{id=my-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:58:24,557] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Revoke previously assigned partitions my_topic_users-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2022-01-07 00:58:24,556] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2022-01-07 00:58:24,558] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Member connector-consumer-my-sink-connect-0-12f96550-9d61-44b8-9399-08f2fdbc74ad sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2022-01-07 00:58:24,557] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:149)
[2022-01-07 00:58:24,557] INFO Scheduled shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:58:24,559] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:58:24,560] INFO Completed shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:58:24,563] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,563] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,564] INFO Completed shutdown for WorkerConnector{id=my-source-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:58:24,564] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,566] INFO App info kafka.consumer for connector-consumer-my-sink-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,587] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2022-01-07 00:58:24,588] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:58:24,590] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:58:24,590] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:58:24,590] INFO [Producer clientId=connector-producer-my-source-connect-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1205)
[2022-01-07 00:58:24,591] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,592] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,592] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,592] INFO App info kafka.producer for connector-producer-my-source-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,593] INFO [Worker clientId=connect-1, groupId=connect-cluster] Member connect-1-4a35edd6-bbcd-4ee1-aa38-39c19497e3b1 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2022-01-07 00:58:24,594] WARN [Worker clientId=connect-1, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1010)
[2022-01-07 00:58:24,595] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,595] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,595] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,596] INFO App info kafka.connect for connect-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,596] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:170)
[2022-01-07 00:58:24,596] INFO [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1205)
[2022-01-07 00:58:24,597] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,598] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,598] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,598] INFO App info kafka.producer for producer-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,598] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,598] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,598] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,599] INFO App info kafka.consumer for consumer-connect-cluster-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,599] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:196)
[2022-01-07 00:58:24,599] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:285)
[2022-01-07 00:58:24,599] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:170)
[2022-01-07 00:58:24,599] INFO [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1205)
[2022-01-07 00:58:24,600] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,600] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,601] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,601] INFO App info kafka.producer for producer-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,601] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,601] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,601] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,602] INFO App info kafka.consumer for consumer-connect-cluster-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,602] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:196)
[2022-01-07 00:58:24,602] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:287)
[2022-01-07 00:58:24,602] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:209)
[2022-01-07 00:58:24,603] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:134)
[2022-01-07 00:58:24,603] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:170)
[2022-01-07 00:58:24,603] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1205)
[2022-01-07 00:58:24,604] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,604] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,604] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,604] INFO App info kafka.producer for producer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,605] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,605] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,606] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,607] INFO App info kafka.consumer for consumer-connect-cluster-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,607] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:196)
[2022-01-07 00:58:24,607] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:136)
[2022-01-07 00:58:24,607] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:24,607] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:24,607] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:24,607] INFO App info kafka.connect for 127.0.0.1:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:24,607] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:230)
[2022-01-07 00:58:24,608] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:300)
[2022-01-07 00:58:24,609] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:702)
[2022-01-07 00:58:24,609] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2022-01-07 00:58:34,406] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../logs, -Dlog4j.configuration=file:./bin/../etc/kafka/connect-log4j.properties
	jvm.spec = Homebrew, OpenJDK 64-Bit Server VM, 17.0.1, 17.0.1+1
	jvm.classpath = /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-transforms-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/zstd-jni-1.4.5-6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-core-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-api-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/zookeeper-3.5.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/rocksdbjni-5.18.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jopt-simple-5.0.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-log4j-appender-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-io-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-runtime-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/reflections-0.9.12.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/slf4j-log4j12-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs-test-sources.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-codec-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-mirror-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-common-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-server-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-mirror-client-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-util-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/argparse4j-0.7.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/activation-1.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-servlets-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-clients-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-logging_2.13-3.9.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/slf4j-api-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/javassist-3.25.0-GA.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-reflect-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-media-jaxb-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/maven-artifact-3.6.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-streams-test-utils-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-library-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-continuation-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/hk2-locator-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-collection-compat_2.13-2.2.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-module-paranamer-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/plexus-utils-3.2.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-server-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/aopalliance-repackaged-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/zookeeper-jute-3.5.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-transport-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-streams-examples-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/commons-cli-1.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/paranamer-2.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-tools-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-common-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-streams-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jaxb-api-2.3.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-container-servlet-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/metrics-core-2.2.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-client-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-streams-scala_2.13-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.activation-api-1.2.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/lz4-java-1.7.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-http-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/osgi-resource-locator-1.0.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-container-servlet-core-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.inject-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/audience-annotations-0.5.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/hk2-utils-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-resolver-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-hk2-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/hk2-api-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-handler-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-databind-2.10.5.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-basic-auth-extension-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-transport-native-epoll-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-security-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.annotation-api-1.3.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/snappy-java-1.1.7.7.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-transport-native-unix-common-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs-sources.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-json-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jetty-servlet-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/mariadb-java-client-2.7.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs-javadoc.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/connect-file-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jersey-client-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka-raft-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/netty-buffer-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/kafka_2.13-6.1.0-ccs-test.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/javassist-3.26.0-GA.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jakarta.validation-api-2.0.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka/commons-lang3-3.8.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/common-config-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/common-metrics-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/slf4j-api-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/build-tools-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/confluent-common/common-utils-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-protobuf-provider-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-streams-avro-serde-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-logging-1.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-core-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlinx-coroutines-core-1.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-stdlib-1.4.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/rocksdbjni-5.18.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/gson-2.8.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-reflect-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-scripting-common-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jersey-common-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/protobuf-java-3.11.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/joda-time-2.9.9.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/json-20190722.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-schema-registry-client-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/slf4j-api-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-json-schema-provider-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/wire-runtime-3.2.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/animal-sniffer-annotations-1.18.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/wire-schema-3.2.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-connect-protobuf-converter-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/org.everit.json.schema-1.12.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/scala-library-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-connect-json-schema-converter-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/protobuf-java-util-3.11.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-connect-avro-data-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jakarta.ws.rs-api-2.1.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-json-schema-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-validator-1.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/failureaccess-1.0.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/okio-2.5.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/annotations-13.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-datatype-guava-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/error_prone_annotations-2.3.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-streams-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-protobuf-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-schema-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/swagger-annotations-1.6.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/avro-1.9.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/handy-uri-templates-2.1.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/osgi-resource-locator-1.0.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/validation-api-2.0.1.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-datatype-jdk8-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-connect-avro-converter-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jakarta.inject-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/checker-qual-2.8.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-module-parameter-names-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-avro-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-collections-3.2.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-streams-protobuf-serde-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-compress-1.19.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-databind-2.10.5.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-streams-json-schema-serde-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-scripting-jvm-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jakarta.annotation-api-1.3.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/j2objc-annotations-1.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/classgraph-4.8.21.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-stdlib-jdk7-1.3.71.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kafka-json-serializer-6.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-script-runtime-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/re2j-1.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-scripting-compiler-embeddable-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-stdlib-common-1.3.71.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-stdlib-jdk8-1.3.71.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jsr305-3.0.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-datatype-joda-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/jackson-datatype-jsr310-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/guava-28.1-jre.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/share/java/kafka-serde-tools/commons-digester-1.8.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-transforms-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/zstd-jni-1.4.5-6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-core-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-api-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/zookeeper-3.5.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/rocksdbjni-5.18.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-log4j-appender-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-io-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-runtime-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/reflections-0.9.12.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs-test-sources.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-codec-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-mirror-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-common-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-server-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-mirror-client-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-util-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-servlets-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-clients-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-logging_2.13-3.9.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/javassist-3.25.0-GA.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-reflect-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-media-jaxb-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-streams-test-utils-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-library-2.13.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-continuation-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-collection-compat_2.13-2.2.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-module-paranamer-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-server-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-transport-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-streams-examples-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-tools-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-common-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-streams-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-container-servlet-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-client-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-streams-scala_2.13-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/lz4-java-1.7.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-http-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-container-servlet-core-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-resolver-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-hk2-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/hk2-api-2.6.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-handler-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-databind-2.10.5.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-basic-auth-extension-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-security-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/snappy-java-1.1.7.7.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs-sources.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-json-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jetty-servlet-9.4.33.v20201020.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/mariadb-java-client-2.7.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs-javadoc.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/connect-file-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jersey-client-2.31.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka-raft-6.1.0-ccs.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/netty-buffer-4.1.51.Final.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/kafka_2.13-6.1.0-ccs-test.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluent-6.1.0/bin/../share/java/confluent-telemetry/*
	os.spec = Mac OS X, x86_64, 12.0.1
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2022-01-07 00:58:34,431] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed:90)
[2022-01-07 00:58:34,446] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/checker-qual-3.5.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,516] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/checker-qual-3.5.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,517] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:34,517] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:34,517] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:34,522] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/osdt_cert-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,543] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/osdt_cert-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,544] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/xmlparserv2-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,644] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/xmlparserv2-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,645] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ons-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,658] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ons-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,659] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/slf4j-api-1.7.30.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,665] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/slf4j-api-1.7.30.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,665] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/oraclepki-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,687] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/oraclepki-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,688] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/simplefan-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,694] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/simplefan-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,694] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/postgresql-42.2.19.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,755] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/postgresql-42.2.19.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,758] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/xdb-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,771] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/xdb-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,772] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/jtds-1.3.1.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,786] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/jtds-1.3.1.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,788] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ucp-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,861] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ucp-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,862] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/kafka-connect-jdbc-10.2.6.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,881] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/kafka-connect-jdbc-10.2.6.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,881] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:34,881] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:34,881] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/common-utils-6.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:34,885] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/common-utils-6.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:34,886] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ojdbc8-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:35,070] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/ojdbc8-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:35,103] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/osdt_core-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:35,122] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/osdt_core-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:35,123] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/sqlite-jdbc-3.25.2.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:35,136] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/sqlite-jdbc-3.25.2.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:35,139] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/orai18n-19.7.0.0.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:35,148] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/orai18n-19.7.0.0.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:35,148] INFO Loading plugin from: /Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/mssql-jdbc-8.4.1.jre8.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2022-01-07 00:58:35,286] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib/mssql-jdbc-8.4.1.jre8.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:36,771] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@251a69d7 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2022-01-07 00:58:36,771] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,772] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,772] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,772] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,772] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,772] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,772] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,772] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,773] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,773] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,779] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,779] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,779] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,779] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,779] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,779] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,780] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,780] INFO Added plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,780] INFO Added plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,780] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,780] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,780] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,780] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,781] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,781] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,781] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,781] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,781] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,781] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,782] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,782] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,782] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,782] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,782] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,782] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,783] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,784] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,784] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,784] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,784] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,784] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,784] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,784] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2022-01-07 00:58:36,786] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,786] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,786] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,787] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,787] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,787] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,787] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,787] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,787] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,788] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,788] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,788] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,788] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,789] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,789] INFO Added aliases 'JsonSchemaConverter' and 'JsonSchema' to plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,789] INFO Added aliases 'ProtobufConverter' and 'Protobuf' to plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,789] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,789] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,789] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,790] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,790] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,790] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,790] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,791] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,791] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,791] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,791] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,792] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,792] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,792] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,792] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,793] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,793] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,793] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,793] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,793] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,794] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,794] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,794] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,794] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,794] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,794] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2022-01-07 00:58:36,794] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,794] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,794] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2022-01-07 00:58:36,850] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.path = [/Users/bigpenguin/project/Inflearn-Spring-Cloud-MSA/kafka_utils/confluentinc-kafka-connect-jdbc-10.2.6/lib]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:361)
[2022-01-07 00:58:36,853] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:58:36,859] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:36,925] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,925] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,925] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,925] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,926] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:36,927] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:36,927] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:36,927] INFO Kafka startTimeMs: 1641484716926 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,214] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:58:37,215] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,224] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,224] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,224] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,234] INFO Logging initialized @3410ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2022-01-07 00:58:37,276] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2022-01-07 00:58:37,277] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2022-01-07 00:58:37,285] INFO jetty-9.4.33.v20201020; built: 2020-10-20T23:39:24.803Z; git: 1be68755656cef678b79a2ef1c2ebbca99e25420; jvm 17.0.1+1 (org.eclipse.jetty.server.Server:375)
[2022-01-07 00:58:37,313] INFO Started http_8083@573284a5{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2022-01-07 00:58:37,313] INFO Started @3489ms (org.eclipse.jetty.server.Server:415)
[2022-01-07 00:58:37,333] INFO Advertised URI: http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2022-01-07 00:58:37,333] INFO REST server listening at http://127.0.0.1:8083/, advertising URL http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2022-01-07 00:58:37,337] INFO Advertised URI: http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2022-01-07 00:58:37,338] INFO REST admin endpoints at http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2022-01-07 00:58:37,338] INFO Advertised URI: http://127.0.0.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2022-01-07 00:58:37,339] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:58:37,341] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,348] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,348] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,348] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,348] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,348] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,349] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,349] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,349] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,349] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,349] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,349] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,350] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,350] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,351] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,351] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,351] INFO Kafka startTimeMs: 1641484717350 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,367] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:58:37,367] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,371] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,371] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,372] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,377] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2022-01-07 00:58:37,383] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:58:37,384] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,388] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,388] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,388] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,389] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,390] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,390] INFO Kafka startTimeMs: 1641484717389 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,399] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:58:37,399] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,401] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,402] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,402] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,405] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,405] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,405] INFO Kafka startTimeMs: 1641484717405 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,523] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:58:37,525] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:58:37,525] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:58:37,526] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,530] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,530] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,530] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,530] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,530] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,530] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,530] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,530] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,530] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,531] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,531] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,531] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,531] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,531] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,531] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,531] INFO Kafka startTimeMs: 1641484717531 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,542] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:58:37,542] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,544] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,544] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,544] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,550] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:58:37,550] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,553] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,553] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,553] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,554] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,554] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,554] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,554] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,554] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,554] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,554] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,554] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,555] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,555] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,555] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,555] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,555] INFO Kafka startTimeMs: 1641484717555 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,565] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:58:37,565] INFO App info kafka.admin.client for adminclient-5 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,567] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,568] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,568] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,571] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:58:37,571] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,574] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,575] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,576] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,576] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,576] INFO Kafka startTimeMs: 1641484717576 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,585] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:58:37,585] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,587] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,587] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,587] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,598] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-01-07 00:58:37,599] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,602] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,602] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,602] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,602] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,603] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,604] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,604] INFO Kafka startTimeMs: 1641484717603 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,613] INFO Kafka cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-01-07 00:58:37,613] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,615] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,615] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,615] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,642] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,643] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,643] INFO Kafka startTimeMs: 1641484717642 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,645] INFO Kafka Connect distributed worker initialization took 3214ms (org.apache.kafka.connect.cli.ConnectDistributed:128)
[2022-01-07 00:58:37,645] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2022-01-07 00:58:37,648] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2022-01-07 00:58:37,648] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:287)
[2022-01-07 00:58:37,650] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2022-01-07 00:58:37,650] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:127)
[2022-01-07 00:58:37,650] INFO Starting KafkaBasedLog with topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:128)
[2022-01-07 00:58:37,651] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,654] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,654] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,654] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,655] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,656] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,656] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,656] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,656] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,656] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,656] INFO Kafka startTimeMs: 1641484717656 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,681] INFO App info kafka.admin.client for adminclient-8 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,682] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,683] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,683] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,688] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:361)
[2022-01-07 00:58:37,696] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2022-01-07 00:58:37,704] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,705] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,706] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,706] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,706] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,706] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,706] INFO Kafka startTimeMs: 1641484717706 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,711] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:58:37,712] INFO [Producer clientId=producer-1] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:37,729] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,729] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,729] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,729] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,729] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,729] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,729] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,729] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,729] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,730] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,730] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,730] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,730] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,730] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,730] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,730] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,730] INFO Kafka startTimeMs: 1641484717730 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,736] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:37,740] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Subscribed to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer:1116)
[2022-01-07 00:58:37,742] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,743] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,743] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,743] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,743] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,743] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,744] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,745] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,745] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,745] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,745] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,767] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2022-01-07 00:58:37,767] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2022-01-07 00:58:37,768] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2022-01-07 00:58:37,771] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,772] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,772] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,772] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,773] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,773] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,773] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,773] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,773] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,773] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,773] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,774] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,774] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,774] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,774] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,774] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,774] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,774] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,775] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,776] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,826] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:164)
[2022-01-07 00:58:37,826] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:166)
[2022-01-07 00:58:37,827] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:129)
[2022-01-07 00:58:37,828] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2022-01-07 00:58:37,829] INFO Starting KafkaBasedLog with topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:128)
[2022-01-07 00:58:37,829] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,831] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,831] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,831] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,831] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,831] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,831] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,831] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,831] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,831] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,832] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,832] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,832] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,832] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,832] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,832] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,832] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,832] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,832] INFO Kafka startTimeMs: 1641484717832 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,845] INFO App info kafka.admin.client for adminclient-9 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,846] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,846] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,846] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,847] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:361)
[2022-01-07 00:58:37,850] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,850] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,850] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,850] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,850] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,850] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,850] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,850] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,850] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,851] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,851] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,851] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,851] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,851] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,851] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,852] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,852] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,852] INFO Kafka startTimeMs: 1641484717852 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,853] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:58:37,855] INFO [Producer clientId=producer-2] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:37,857] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,857] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,858] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,858] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,858] INFO Kafka startTimeMs: 1641484717858 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,862] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:37,863] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Subscribed to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer:1116)
[2022-01-07 00:58:37,864] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,864] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,864] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,864] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,864] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,872] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,872] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,873] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,873] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,873] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,907] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:164)
[2022-01-07 00:58:37,907] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:166)
[2022-01-07 00:58:37,909] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:265)
[2022-01-07 00:58:37,909] INFO Starting KafkaBasedLog with topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:128)
[2022-01-07 00:58:37,909] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:361)
[2022-01-07 00:58:37,911] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,911] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,911] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:369)
[2022-01-07 00:58:37,912] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,912] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,912] INFO Kafka startTimeMs: 1641484717912 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,924] INFO App info kafka.admin.client for adminclient-10 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:37,925] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:37,926] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:37,926] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:37,926] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:361)
[2022-01-07 00:58:37,946] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,946] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,946] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,946] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,946] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,946] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,947] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,947] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,947] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,947] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,947] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,947] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,947] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,947] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,948] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:37,948] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,948] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,948] INFO Kafka startTimeMs: 1641484717948 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,948] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:58:37,949] INFO [Producer clientId=producer-3] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:37,953] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,953] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,953] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,953] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,953] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,953] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,954] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,954] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,955] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,955] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,955] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,955] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,955] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,956] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:37,957] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:37,957] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:37,957] INFO Kafka startTimeMs: 1641484717956 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:37,962] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:37,962] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Subscribed to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1116)
[2022-01-07 00:58:37,963] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:618)
[2022-01-07 00:58:37,969] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:37,978] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:58:37,979] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:58:37,979] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:58:37,980] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:164)
[2022-01-07 00:58:37,980] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:166)
[2022-01-07 00:58:37,980] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:280)
[2022-01-07 00:58:37,980] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:291)
[2022-01-07 00:58:37,986] INFO [Worker clientId=connect-1, groupId=connect-cluster] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:37,987] INFO [Worker clientId=connect-1, groupId=connect-cluster] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:58:37,989] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:58:37,989] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:58:37,996] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:58:37,999] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=22, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:58:38,022] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=22, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:58:38,023] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 22 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', leaderUrl='http://127.0.0.1:8083/', offset=36, connectorIds=[my-sink-connect, my-source-connect, my-order-sink-connect], taskIds=[my-sink-connect-0, my-source-connect-0, my-order-sink-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:58:38,025] WARN [Worker clientId=connect-1, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1095)
[2022-01-07 00:58:38,025] INFO [Worker clientId=connect-1, groupId=connect-cluster] Current config state offset -1 is behind group assignment 36, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1158)
[2022-01-07 00:58:38,270] INFO Started o.e.j.s.ServletContextHandler@1a96d94c{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:916)
[2022-01-07 00:58:38,270] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2022-01-07 00:58:38,270] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2022-01-07 00:58:38,477] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 36 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1162)
[2022-01-07 00:58:38,477] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 36 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:58:38,478] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:58:38,479] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-source-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:58:38,479] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-sink-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:58:38,478] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:58:38,479] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-source-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:58:38,479] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:58:38,483] INFO Creating task my-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:58:38,483] INFO Creating connector my-order-sink-connect of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:58:38,483] INFO Creating task my-source-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:58:38,483] INFO Creating connector my-source-connect of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:58:38,483] INFO Creating connector my-sink-connect of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:58:38,483] INFO Creating task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:58:38,485] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:58:38,485] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:58:38,485] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:361)
[2022-01-07 00:58:38,485] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:58:38,485] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:58:38,486] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:58:38,487] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,487] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,487] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,487] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,487] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,487] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,491] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:58:38,491] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:58:38,492] INFO Instantiated connector my-source-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:58:38,492] INFO Instantiated connector my-sink-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:58:38,492] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:58:38,492] INFO Finished creating connector my-source-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:58:38,492] INFO Instantiated task my-order-sink-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:58:38,492] INFO Instantiated task my-sink-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:58:38,493] INFO Instantiated task my-source-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:58:38,493] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:58:38,493] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:58:38,493] INFO Finished creating connector my-sink-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:58:38,492] INFO Instantiated connector my-order-sink-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:58:38,493] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:58:38,493] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:58:38,494] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:58:38,493] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:58:38,493] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2022-01-07 00:58:38,494] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:58:38,494] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:58:38,494] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:58:38,494] INFO Finished creating connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:58:38,494] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-source-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:58:38,495] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:58:38,495] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:58:38,495] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:58:38,495] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-source-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:58:38,496] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-source-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:58:38,498] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:361)
[2022-01-07 00:58:38,499] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2022-01-07 00:58:38,499] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,499] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2022-01-07 00:58:38,500] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:58:38,500] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,500] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:58:38,501] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:38,501] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:606)
[2022-01-07 00:58:38,506] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-my-source-connect-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:361)
[2022-01-07 00:58:38,507] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-my-sink-connect-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-sink-connect
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:58:38,506] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-my-order-sink-connect-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-order-sink-connect
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:58:38,511] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:38,512] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:369)
[2022-01-07 00:58:38,512] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:38,512] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:38,512] INFO Kafka startTimeMs: 1641484718512 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:38,512] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	dialect.name = 
	incrementing.column.name = id
	mode = incrementing
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [users]
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = my_topic_
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:361)
[2022-01-07 00:58:38,514] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:38,514] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:38,514] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:38,514] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:38,514] INFO Kafka startTimeMs: 1641484718514 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:38,515] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:58:38,515] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:38,515] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:58:38,515] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:58:38,515] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:58:38,516] INFO Kafka startTimeMs: 1641484718515 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:58:38,517] INFO [Producer clientId=connector-producer-my-source-connect-0] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:38,520] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2022-01-07 00:58:38,520] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:58:38,520] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	dialect.name = 
	incrementing.column.name = id
	mode = incrementing
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 5000
	query = 
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = [users]
	tables = [`inflearn_spring_cloud`.`users`]
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = my_topic_
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:361)
[2022-01-07 00:58:38,521] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2022-01-07 00:58:38,521] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Subscribed to topic(s): orders (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2022-01-07 00:58:38,522] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Subscribed to topic(s): my_topic_users (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2022-01-07 00:58:38,522] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2022-01-07 00:58:38,522] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2022-01-07 00:58:38,526] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:361)
[2022-01-07 00:58:38,526] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:361)
[2022-01-07 00:58:38,529] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:58:38,529] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:58:38,530] INFO WorkerSinkTask{id=my-sink-connect-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2022-01-07 00:58:38,530] INFO WorkerSinkTask{id=my-order-sink-connect-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2022-01-07 00:58:38,533] INFO [Worker clientId=connect-1, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1578)
[2022-01-07 00:58:38,536] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:38,537] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:58:38,537] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:58:38,537] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:58:38,538] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:58:38,539] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:58:38,547] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:58:38,547] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:58:38,550] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-my-order-sink-connect-0-68efa34f-0734-4f55-8132-9b5c3763230f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:58:38,551] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-my-sink-connect-0-aa47e6bb-af4a-46ef-b4e7-e7f70b19f0d9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:58:38,552] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Finished assignment for group at generation 3: {connector-consumer-my-order-sink-connect-0-68efa34f-0734-4f55-8132-9b5c3763230f=Assignment(partitions=[orders-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:58:38,552] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Finished assignment for group at generation 5: {connector-consumer-my-sink-connect-0-aa47e6bb-af4a-46ef-b4e7-e7f70b19f0d9=Assignment(partitions=[my_topic_users-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:58:38,558] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-my-order-sink-connect-0-68efa34f-0734-4f55-8132-9b5c3763230f', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:58:38,558] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-my-sink-connect-0-aa47e6bb-af4a-46ef-b4e7-e7f70b19f0d9', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:58:38,558] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Notifying assignor about the new Assignment(partitions=[my_topic_users-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:58:38,558] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Notifying assignor about the new Assignment(partitions=[orders-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:58:38,558] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Adding newly assigned partitions: my_topic_users-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:58:38,559] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Adding newly assigned partitions: orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:58:38,562] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Found no committed offset for partition orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1354)
[2022-01-07 00:58:38,563] INFO [Consumer clientId=connector-consumer-my-sink-connect-0, groupId=connect-my-sink-connect] Setting offset for partition my_topic_users-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2022-01-07 00:58:38,566] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Resetting offset for partition orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:58:38,579] INFO Starting thread to monitor tables. (io.confluent.connect.jdbc.source.TableMonitorThread:73)
[2022-01-07 00:58:38,581] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Error converting message value in topic 'orders' partition 0 at offset 13 and timestamp 1641483815977: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration. (org.apache.kafka.connect.runtime.WorkerSinkTask:547)
org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:370)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertValue(WorkerSinkTask.java:545)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:478)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:328)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-01-07 00:58:38,586] ERROR WorkerSinkTask{id=my-order-sink-connect-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:206)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:478)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:328)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:370)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertValue(WorkerSinkTask.java:545)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:501)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	... 13 more
[2022-01-07 00:58:38,587] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2022-01-07 00:58:38,587] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Revoke previously assigned partitions orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2022-01-07 00:58:38,588] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Member connector-consumer-my-order-sink-connect-0-68efa34f-0734-4f55-8132-9b5c3763230f sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1029)
[2022-01-07 00:58:38,592] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:668)
[2022-01-07 00:58:38,593] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:672)
[2022-01-07 00:58:38,593] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:678)
[2022-01-07 00:58:38,595] INFO App info kafka.consumer for connector-consumer-my-order-sink-connect-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-01-07 00:58:38,833] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:58:38,874] INFO Found offset {{table=users}=null, {protocol=1, table=inflearn_spring_cloud.users}={incrementing=8}} for partition {protocol=1, table=inflearn_spring_cloud.users} (io.confluent.connect.jdbc.source.JdbcSourceTask:196)
[2022-01-07 00:58:38,876] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2022-01-07 00:58:38,876] INFO WorkerSourceTask{id=my-source-connect-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)
[2022-01-07 00:58:38,879] INFO Begin using SQL query: SELECT * FROM `inflearn_spring_cloud`.`users` WHERE `inflearn_spring_cloud`.`users`.`id` > ? ORDER BY `inflearn_spring_cloud`.`users`.`id` ASC (io.confluent.connect.jdbc.source.TableQuerier:164)
[2022-01-07 00:58:39,036] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:58:39,037] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-sink-connect
	predicates = []
	tasks.max = 1
	topics = [my_topic_users]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:39,037] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2022-01-07 00:58:39,038] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:58:39,038] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:39,038] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2022-01-07 00:58:39,100] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:361)
[2022-01-07 00:58:39,100] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:39,102] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:361)
[2022-01-07 00:58:39,102] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-source-connect
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:58:48,522] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:58:48,523] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:58:58,526] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:58:58,526] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:59:07,437] INFO Successfully processed removal of connector 'my-order-sink-connect' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2022-01-07 00:59:07,438] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1521)
[2022-01-07 00:59:07,439] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling connector-only config update by stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:593)
[2022-01-07 00:59:07,439] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:59:07,439] INFO Scheduled shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2022-01-07 00:59:07,439] INFO Completed shutdown for WorkerConnector{id=my-order-sink-connect} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2022-01-07 00:59:07,440] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:59:07,440] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:59:07,443] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=23, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:59:07,447] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=23, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:59:07,452] INFO Stopping connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:387)
[2022-01-07 00:59:07,453] WARN Ignoring stop request for unowned connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:390)
[2022-01-07 00:59:07,453] INFO Stopping task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:836)
[2022-01-07 00:59:07,453] WARN Ignoring await stop request for non-present connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:415)
[2022-01-07 00:59:07,454] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1767)
[2022-01-07 00:59:07,460] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1786)
[2022-01-07 00:59:07,460] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 23 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', leaderUrl='http://127.0.0.1:8083/', offset=39, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[my-order-sink-connect], revokedTaskIds=[my-order-sink-connect-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:59:07,461] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 39 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:59:07,461] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:59:07,461] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:59:07,461] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:59:07,463] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=24, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:59:07,466] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=24, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:59:07,466] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 24 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', leaderUrl='http://127.0.0.1:8083/', offset=39, connectorIds=[my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:59:07,467] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 39 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:59:07,467] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:59:08,530] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:59:08,531] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:59:16,856] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:361)
[2022-01-07 00:59:16,862] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector my-order-sink-connect config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1534)
[2022-01-07 00:59:16,862] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:59:16,862] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:59:16,866] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=25, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:59:16,869] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=25, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:59:16,869] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 25 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', leaderUrl='http://127.0.0.1:8083/', offset=40, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:59:16,870] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 40 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:59:16,870] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector my-order-sink-connect (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1299)
[2022-01-07 00:59:16,870] INFO Creating connector my-order-sink-connect of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2022-01-07 00:59:16,871] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:59:16,871] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:59:16,873] INFO Instantiated connector my-order-sink-connect with version 10.2.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2022-01-07 00:59:16,873] INFO Finished creating connector my-order-sink-connect (org.apache.kafka.connect.runtime.Worker:310)
[2022-01-07 00:59:16,873] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:59:16,874] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:59:16,874] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:59:16,875] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2022-01-07 00:59:17,372] INFO [Worker clientId=connect-1, groupId=connect-cluster] Tasks [my-order-sink-connect-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1549)
[2022-01-07 00:59:17,372] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder:647)
[2022-01-07 00:59:17,373] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2022-01-07 00:59:17,373] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:59:17,375] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=26, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:59:17,377] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=26, memberId='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:59:17,378] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 26 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-4840f7ed-5143-4bba-80d6-b507d97545fc', leaderUrl='http://127.0.0.1:8083/', offset=42, connectorIds=[my-order-sink-connect, my-sink-connect, my-source-connect], taskIds=[my-order-sink-connect-0, my-sink-connect-0, my-source-connect-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1689)
[2022-01-07 00:59:17,378] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 42 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1216)
[2022-01-07 00:59:17,378] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1258)
[2022-01-07 00:59:17,379] INFO Creating task my-order-sink-connect-0 (org.apache.kafka.connect.runtime.Worker:509)
[2022-01-07 00:59:17,379] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:361)
[2022-01-07 00:59:17,379] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:59:17,379] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:361)
[2022-01-07 00:59:17,379] INFO Instantiated task my-order-sink-connect-0 with version 10.2.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2022-01-07 00:59:17,380] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:59:17,380] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2022-01-07 00:59:17,380] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:361)
[2022-01-07 00:59:17,380] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2022-01-07 00:59:17,380] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task my-order-sink-connect-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2022-01-07 00:59:17,381] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:632)
[2022-01-07 00:59:17,381] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:361)
[2022-01-07 00:59:17,381] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = my-order-sink-connect
	predicates = []
	tasks.max = 1
	topics = [orders]
	topics.regex = 
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:361)
[2022-01-07 00:59:17,382] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-my-order-sink-connect-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-my-order-sink-connect
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:361)
[2022-01-07 00:59:17,384] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:59:17,385] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:369)
[2022-01-07 00:59:17,385] INFO Kafka version: 6.1.0-ccs (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-01-07 00:59:17,385] INFO Kafka commitId: 5496d92defc9bbe4 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-01-07 00:59:17,385] INFO Kafka startTimeMs: 1641484757385 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-01-07 00:59:17,386] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1244)
[2022-01-07 00:59:17,386] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Subscribed to topic(s): orders (org.apache.kafka.clients.consumer.KafkaConsumer:961)
[2022-01-07 00:59:17,386] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2022-01-07 00:59:17,386] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = true
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3307/inflearn_spring_cloud
	connection.user = big
	db.timezone = UTC
	delete.enabled = false
	dialect.name = 
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:361)
[2022-01-07 00:59:17,387] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:59:17,387] INFO WorkerSinkTask{id=my-order-sink-connect-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:309)
[2022-01-07 00:59:17,390] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Cluster ID: 72UjNreVRAqSM6a74u5ERg (org.apache.kafka.clients.Metadata:279)
[2022-01-07 00:59:17,390] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:847)
[2022-01-07 00:59:17,391] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:59:17,393] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2022-01-07 00:59:17,395] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-my-order-sink-connect-0-cfb6b190-47e8-4904-965a-ae2c6dd5fb74', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2022-01-07 00:59:17,395] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Finished assignment for group at generation 1: {connector-consumer-my-order-sink-connect-0-cfb6b190-47e8-4904-965a-ae2c6dd5fb74=Assignment(partitions=[orders-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2022-01-07 00:59:17,397] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-my-order-sink-connect-0-cfb6b190-47e8-4904-965a-ae2c6dd5fb74', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:756)
[2022-01-07 00:59:17,398] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Notifying assignor about the new Assignment(partitions=[orders-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2022-01-07 00:59:17,398] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Adding newly assigned partitions: orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2022-01-07 00:59:17,399] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Found no committed offset for partition orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1354)
[2022-01-07 00:59:17,403] INFO [Consumer clientId=connector-consumer-my-order-sink-connect-0, groupId=connect-my-order-sink-connect] Resetting offset for partition orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:396)
[2022-01-07 00:59:18,534] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:59:18,535] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:59:28,537] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:59:28,537] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:59:38,538] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:59:38,538] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:59:46,793] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:59:46,806] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:59:46,830] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:59:46,834] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:59:46,865] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:59:46,868] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:59:46,880] WARN Write of 1 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=113) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=113) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:59:46,882] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:59:46,885] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:59:46,885] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=113) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=113) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=113) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=113) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:59:47,385] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:59:47,392] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:59:47,403] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:59:47,405] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:59:47,429] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:59:47,432] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:59:47,436] WARN Write of 1 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=115) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=115) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:59:47,437] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:59:47,439] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:59:47,439] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=115) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=115) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=115) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=115) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:59:48,543] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:59:48,544] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2022-01-07 00:59:50,441] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:59:50,448] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:59:50,458] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:59:50,460] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:59:50,480] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:59:50,482] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:59:50,487] WARN Write of 1 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=117) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=117) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:59:50,488] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:59:50,490] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:59:50,490] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=117) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=117) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=117) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=117) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:59:53,491] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:59:53,500] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:59:53,511] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:59:53,514] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:59:53,541] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:59:53,544] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:59:53,549] WARN Write of 1 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=119) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=119) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:59:53,550] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:59:53,552] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:59:53,552] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=119) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=119) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=119) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=119) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:59:56,553] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:59:56,566] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:59:56,581] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:59:56,583] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:59:56,603] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:59:56,605] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:59:56,608] WARN Write of 1 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=121) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=121) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:59:56,609] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:59:56,611] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:59:56,611] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=121) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=121) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=121) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=121) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:59:57,441] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2022-01-07 00:59:57,454] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2022-01-07 00:59:57,467] INFO Checking MySql dialect for existence of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2022-01-07 00:59:57,470] INFO Using MySql dialect TABLE "orders" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2022-01-07 00:59:57,494] INFO Checking MySql dialect for type of TABLE "orders" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2022-01-07 00:59:57,497] INFO Setting metadata for table "orders" to Table{name='"orders"', type=TABLE columns=[Column{'product_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'qty', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'total_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}, Column{'id', isPrimaryKey=true, allowsNull=false, sqlType=INT}, Column{'user_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'created_at', isPrimaryKey=false, allowsNull=true, sqlType=DATETIME}, Column{'order_id', isPrimaryKey=false, allowsNull=false, sqlType=VARCHAR}, Column{'unit_price', isPrimaryKey=false, allowsNull=true, sqlType=INT}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2022-01-07 00:59:57,500] WARN Write of 1 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: (conn=123) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)
	at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLTransientConnectionException: (conn=123) Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:79)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)
	at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)
	... 16 more
Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)
	... 4 more
Caused by: java.sql.SQLException: Field 'order_id' doesn't have a default value
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)
	at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)
	... 5 more
[2022-01-07 00:59:57,500] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2022-01-07 00:59:57,502] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2022-01-07 00:59:57,502] ERROR WorkerSinkTask{id=my-order-sink-connect-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:605)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=123) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=123) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:586)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:234)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: (conn=123) Field 'order_id' doesn't have a default value
java.sql.SQLTransientConnectionException: (conn=123) Field 'order_id' doesn't have a default value
org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Field 'order_id' doesn't have a default value
java.sql.SQLException: Field 'order_id' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2022-01-07 00:59:58,549] INFO WorkerSourceTask{id=my-source-connect-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2022-01-07 00:59:58,550] INFO WorkerSourceTask{id=my-source-connect-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
